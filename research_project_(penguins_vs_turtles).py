# -*- coding: utf-8 -*-
"""Research_Project_(penguins_vs_turtles).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ov9aoCrTApN7Iz6M2-zdGiPE2mi_cJsv
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q keras
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D
from keras.models import Model
from keras.optimizers import Adam

train_data_dir = '/content/drive/MyDrive/archive/train'
validation_data_dir = '/content/drive/MyDrive/archive/valid'

img_width = 299
img_height = 299

train_samples = 65
validation_samples = 10
epochs = 20
batch_size = 5

from pickle import TRUE
# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator

train_datagen = ImageDataGenerator (
    rescale = 1./255 ,
    shear_range = 0.2,
    zoom_range = 0.2,
    rotation_range = 20,
    width_shift_range = 0.2,
    height_shift_range = 0.2,
    horizontal_flip = True
)

val_datagen = ImageDataGenerator (
    rescale = 1.0/255
)

# https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/#h-flow-from-directory
train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size = (img_height, img_width),
    batch_size = batch_size,
    class_mode = 'binary')

validation_generator = val_datagen.flow_from_directory(
    validation_data_dir,
    target_size = (img_height, img_width),
    batch_size = batch_size,
    class_mode = 'binary')

base_model = applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))

model_top = Sequential()
model_top.add(GlobalAveragePooling2D(input_shape=base_model.output_shape[1:], data_format=None)),
model_top.add(Dense(256, activation='relu'))
model_top.add(Dropout(0.5))
model_top.add(Dense(1, activation='sigmoid'))

model = Model(inputs=base_model.input, outputs=model_top(base_model.output))

model.compile(optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08,decay=0.0), loss='binary_crossentropy', metrics=['accuracy'])

# https://www.rdocumentation.org/packages/keras/versions/2.11.1/topics/fit_generator
history = model.fit_generator(
    train_generator,
    steps_per_epoch = train_samples//batch_size,
    epochs = epochs,
    validation_data = validation_generator,
    validation_steps = validation_samples // batch_size
)

import matplotlib.pyplot as plt
print(history.history.keys())

# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/
plt.figure()
plt.plot(history.history['accuracy'], 'orange', label='Training accuracy')
plt.plot(history.history['val_accuracy'], 'blue', label='Validation accuracy')
plt.plot(history.history['loss'], 'red', label='Training loss')
plt.plot(history.history['val_loss'], 'green', label='Validation loss')
plt.legend()

import numpy as np
from tensorflow.keras.utils import load_img
from tensorflow.keras.utils import img_to_array

img_path='/content/drive/MyDrive/archive/turtle.jpg' #change to location of abd x-ray
img_path2='/content/drive/MyDrive/archive/penguin.jpeg'  #change to location of chest x-ray
img = load_img(img_path, target_size=(img_width, img_height))
img2 = load_img(img_path2, target_size=(img_width, img_height))
plt.imshow(img)
plt.show()

img = img_to_array(img)
x = np.expand_dims(img, axis=0) * 1./255
score = model.predict(x)
print('Predicted:', score, 'Turtle' if score < 0.99996 else 'Penguin')

plt.imshow(img2)
plt.show()

img = img_to_array(img2)
x = np.expand_dims(img2, axis=0) * 1./255
score2 = model.predict(x)
print('Predicted:', score2, 'Turtle' if score2 < 0.99996 else 'Penguin')